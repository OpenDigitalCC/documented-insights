Ref. Ares(2026)1225929 - 04/02/2026

Contribution to the open source Consultation Call for evidence

Introduction

I am submitting this contribution in a personal capacity as an independent practitioner
with several years of experience working at the intersection of open source and closed-
source software, data infrastructure, and AI-supported knowledge systems.

My background includes hands-on involvement in the design and operation of modular,
auditable software systems in regulated and public-interest contexts, including
education, data governance, and infrastructure-related domains.

This contribution reflects observations derived from practical system design and
implementation experience and does not represent the position of any organization or
employer.

Open source governance architectures for AI-assisted, evidence-grounded
knowledge systems

Context and problem statement

Across public administrations, education systems, and regulated or semi-public
sectors, institutions increasingly rely on authoritative knowledge systems to support
decisions, guidance, training, and compliance. These systemsâsuch as curricula,
certification frameworks, regulatory guidance, standards, and internal proceduresâ
function as institutional reference points and carry formal authority with direct societal
and economic impact.

Recent advances in artificial intelligence make it technically feasible to analyze,
synthesize, and update large volumes of heterogeneous source material at a scale that
exceeds human capacity. In practice, however, most AI deployments in public-interest
contexts remain confined to narrow productivity support or opaque, vendor-controlled
solutions. They rarely integrate into canonical knowledge bases in a way that preserves
accountability, traceability, and institutional responsibility.

The core challenge is therefore not model capability, but system architecture and
governance. Existing knowledge management systems were designed around static,
human-authored artefacts and informal provenance practices. When AI components
are introduced without structural safeguards, they blur responsibility, weaken evidence
chains, and create dependencies on closed, non-inspectable componentsâ
significantly limiting their suitability for reliance in public-sector contexts.

This contribution argues that open source should be understood not merely as a
licensing model, but as a governance and infrastructure principle for AI-assisted
knowledge systems in public-interest environments, enabling auditability, contestability,
and long-term institutional control.
Why open source matters in AI-assisted knowledge work

In regulated knowledge systems and educational contexts, the primary requirement is
not automation, but institutional control: the ability to understand how knowledge is
produced, to trace decisions back to evidence, to correct errors over time, and to retain
accountability with human or legal authorities.

Closed, vertically integrated AI solutions struggle to meet these requirements. They
typically combine opaque decision logic, non-standard interfaces, limited auditability,
and strong vendor lock-in.

Open source approachesâunderstood broadly as openness of process, standards,
interfaces, and governance mechanismsâprovide a structural alternative. They enable
public institutions to:

   â¢   inspect and audit decision pathways,

   â¢   separate AI-generated proposals from authoritative decisions,

   â¢   replace or independently evaluate components,

   â¢   federate systems across organizational boundaries,

   â¢   and avoid long-term dependency on single vendors.

Crucially, openness here does not imply unrestricted modification or lack of control. On
the contrary, open governance mechanisms enable stricter, more enforceable control,
because rules, policies, and decision boundaries are explicit, machine-readable, and
inspectable.

Open source governance requirements for AI-assisted knowledge systems

Based on observed shortcomings in current deployments, the following conditions
should be considered minimum requirements for AI-assisted knowledge systems in
public-interest contexts. These requirements are technology-agnostic and compatible
with different implementation choices, but they depend on open standards and
inspectable system design.

   1. Separation of proposal and authority
      AI systems may generate proposals, summaries, or candidate interpretations,
      but must not directly modify canonical knowledge or trigger binding decisions.
      Decision authority remains explicitly with human or legally designated actors.

   2. Human accountability by design
      Responsibility for accepting, rejecting, or modifying AI-generated proposals must
       be clearly attributable to identifiable human roles or institutional authorities, and
       must not be delegated implicitly to automated systems.

   3. Evidence-grounded knowledge objects
      Knowledge artefacts produced or modified with AI support must retain explicit
      links to source material and supporting evidence, enabling later inspection,
      contestation, and correction.

   4. Traceable decision pathways
      Both human and machine contributions must be recorded through structured,
      append-only provenance logs that support auditability and institutional memory.

   5. Epistemic control and contestability
      Institutions must retain the ability to understand, revise, or withdraw knowledge
      artefacts over time. System design must not obscure how knowledge enters or
      changes within canonical repositories.

   6. Fine-grained authorization and capability control
      AI components should operate under narrowly scoped, task-specific
      permissions, minimizing attack surfaces and reducing systemic risk.

   7. Privacy preservation through data minimization
      Authorization and contextualization should rely on roles, policies, and task
      context rather than excessive collection or centralization of personal data.

   8. Standardized, machine-readable governance policies
      Authorization rules, constraints, and decision policies should be expressed in
      open, machine-readable formats to ensure consistent enforcement and
      interoperability.

   9. Interoperability and replaceability
      System components must be independently auditable and replaceable without
      loss of institutional control, enabling long-term sustainability and vendor
      independence.

These requirements are difficult to enforce in closed systems, but naturally supported
by open, standardized architectures.
Reference Architecture

Illustrative example: curriculum-governed knowledge systems

Curriculum-based education provides a clear illustration of why open source
governance architectures matter. Curricula, learning objectives, and assessment
criteria constitute canonical knowledge structures that evolve over time and carry
formal authority.

In an open, governed architecture:

   â¢   AI systems analyze new source material (e.g. updated regulations, standards, or
       academic guidance) and generate structured, evidence-linked proposals.

   â¢   Automated policy checks verify that proposals comply with predefined
       governance rules.
   â¢   Human authorities review proposals, make explicit decisions, and record
       rationales.

   â¢   Approved changes are incorporated into the canonical knowledge base together
       with their full provenance chain.

This pattern allows institutions to benefit from AI-assisted analysis while preserving
accountability, traceability, and control. Importantly, it relies on open interfaces,
inspectable governance logic, and replaceable components, rather than on trust in a
single vendor system.

The same architectural pattern applies beyond education, including regulatory
guidance, internal procedures, standards interpretation, and other public-interest
knowledge domains.

Relevance for EU open source policy

This contribution suggests that EU open source policy should not focus solely on
promoting specific software projects, but also on encouraging open governance
architectures for AI-assisted knowledge systems, particularly where public authority,
education, or regulation is involved.

In practical terms, this could include:

   â¢   encouraging open standards for provenance, governance policies, and audit logs;

   â¢   supporting reference architectures that separate AI proposal generation from
       authoritative decision-making;

   â¢   promoting procurement criteria that prioritize inspectability, replaceability, and
       interoperability;

   â¢   and recognizing open governance mechanisms as a form of digital public
       infrastructure.

Such an approach strengthens digital sovereignty, reduces long-term systemic risk, and
enables trustworthy adoption of AI in public-interest environments without sacrificing
institutional control.

Architectural limits of centralized verification in open digital ecosystems

Recent policy discussions on access control and verification mechanisms illustrate a
broader architectural challenge for European open digital ecosystems: open source
alone is insufficient when authority is centralized and verification is delegated to opaque
or mutable systems. Even when such mechanisms rely on public providers,
anonymization, or initially open source implementations, there is no durable guarantee
that deployed systems continue to run publicly auditable code or that governance and
data use remain aligned with original policy intent. At scale, this creates systemic risk,
particularly for consumer-facing services where behavioral signals may become linkable
across contexts. From an open ecosystem perspective, this underlines the importance
of architectures that derive authorization from transparent, inspectable policies and
contextual, task-specific capabilities rather than persistent identity verification, thereby
strengthening privacy, resilience, and long-term trust.

Conclusion

AI can significantly support knowledge-intensive work in public and regulated
contextsâbut only if embedded within architectures that preserve accountability,
traceability, and human responsibility. Open source, understood as openness of
process, standards, and governance, provides the structural foundation for such
systems.

By focusing on open governance architecture rather than isolated tools, EU policy can
help ensure that AI-assisted knowledge systems remain trustworthy, auditable, and
aligned with public values over the long term.